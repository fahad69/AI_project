{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4486e22a",
   "metadata": {},
   "source": [
    "# i200693\n",
    "## Fahad Maqbool\n",
    "## AI Project:------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d45f9",
   "metadata": {},
   "source": [
    "## Here are the steps you can follow to use this dataset for sentiment analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd0907",
   "metadata": {},
   "source": [
    "##### 1. Data Preparation: The first step is to download the dataset from the google drive and  preprocess the images and captions. The captions need to be cleaned, converted into lowercase, and tokenized into words. The images should be resized to a standard size and normalized. You can extract visual features from the images using a pre-trained CNN such as ResNet or VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "40cebf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\FAHAD\n",
      "[nltk_data]     MAQBOOL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download NLTK punkt tokenizer data\n",
    "nltk.download('stopwords')  # Download NLTK stopwords data\n",
    "from torch.nn import Embedding\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import classification_report\n",
    "import string\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d742753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/content/drive/MyDrive/sentiment/sentiment.csv')\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if present\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6ddff",
   "metadata": {},
   "source": [
    "# Text processing :-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw text to lowercase\n",
    "df['processed_caption'] = df['raw'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenize the raw text\n",
    "df['tokens_process'] = df['processed_caption'].apply(word_tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens_process'] = df['tokens_process'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "\n",
    "# Pad tokens to the maximum length\n",
    "max_length = df['tokens_process'].apply(len).max()\n",
    "df['tokens_process'] = df['tokens_process'].apply(lambda tokens: tokens + [0] * (max_length - len(tokens)))\n",
    "\n",
    "# Convert tokens to word vectors using Word2Vec\n",
    "sentences = df['tokens_process'].tolist()\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to extract text features from token sequences using Word2Vec model\n",
    "def extract_text_features(token_sequences, model):\n",
    "    text_features = []\n",
    "    for tokens in token_sequences:\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        if vectors:\n",
    "            feature = np.mean(vectors, axis=0)  # Compute the mean of word vectors\n",
    "            text_features.append(feature)\n",
    "        else:\n",
    "            text_features.append(np.zeros(model.vector_size))  # Handle tokens with no word vectors\n",
    "    return np.array(text_features)\n",
    "\n",
    "# Extract text features from token sequences\n",
    "text_features = extract_text_features(df['tokens_process'], model)\n",
    "\n",
    "# Add text features to the DataFrame\n",
    "df['text_feature'] = list(text_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7408ad8",
   "metadata": {},
   "source": [
    "# Image Processing :----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the folder containing the images\n",
    "image_folder = '/content/drive/MyDrive/sentiment/sentiment_images/'\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        # Open the image and convert it to RGB\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Define the image transformation pipeline\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Apply the transformation to the image\n",
    "        image = transform(image)\n",
    "        \n",
    "        # Move the image to the GPU device\n",
    "        image = image.to(device)\n",
    "        \n",
    "        return image\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        # Handle file not found error or image processing error\n",
    "        print(f\"Error processing image at path: {image_path}\")\n",
    "        return None\n",
    "\n",
    "# Create a new column 'image_path' by concatenating the image folder path and the 'filename' column\n",
    "df['image_path'] = image_folder + df['filename']\n",
    "\n",
    "# Apply the 'preprocess_image' function to the 'image_path' column and create a new column 'preprocessed_image'\n",
    "df['preprocessed_image'] = df['image_path'].apply(preprocess_image)\n",
    "\n",
    "# Load the pre-trained ResNet-50 model and move it to the GPU device\n",
    "cnn_model = models.resnet50(pretrained=True)\n",
    "cnn_model = cnn_model.eval()\n",
    "cnn_model = cnn_model.to(device)\n",
    "\n",
    "# Function to extract visual features from an image\n",
    "def extract_visual_features(image):\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = cnn_model(image)\n",
    "    return features.squeeze()\n",
    "\n",
    "# Apply the 'extract_visual_features' function to the 'preprocessed_image' column and create a new column 'visual_features'\n",
    "df['visual_features'] = df['preprocessed_image'].apply(extract_visual_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a85051",
   "metadata": {},
   "source": [
    "# Process data:--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "06d64670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imgid                 0\n",
       "split                 0\n",
       "filename              0\n",
       "successful            0\n",
       "tokens                0\n",
       "word_sentiment        0\n",
       "sentiment             0\n",
       "raw                   0\n",
       "processed_caption     0\n",
       "tokens_process        0\n",
       "text_feature          0\n",
       "image_path            0\n",
       "preprocessed_image    0\n",
       "visual_features       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('process_data.csv')\n",
    "\n",
    "# Drop the 'Unnamed: 0' column\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Convert the 'text_feature' column from string representation to numpy array of float32\n",
    "df['text_feature'] = df['text_feature'].apply(lambda x: np.array(x.strip('[]').split(), dtype=np.float32))\n",
    "\n",
    "# Process the 'visual_features' column\n",
    "df['visual_features'] = df['visual_features'].apply(lambda x: np.array(str(x).split('[')[-1].split(']')[0].split(','), dtype=float))\n",
    "\n",
    "# Convert the 'visual_features' column to torch tensor\n",
    "df['visual_features'] = df['visual_features'].apply(lambda x: torch.tensor(x))\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Check for missing values in the DataFrame\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47da64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the tokens column into a list of token sequences\n",
    "split_tokens = [x[1:-1].split(\", \") for x in df[\"tokens\"].tolist()]\n",
    "\n",
    "# Remove the square brackets from individual tokens\n",
    "split_tokens = [[y[1:-1] for y in x] for x in split_tokens]\n",
    "\n",
    "# Create a new column 'tokens_words' to store the token sequences\n",
    "df['tokens_words'] = split_tokens\n",
    "\n",
    "# Find the maximum length of token sequences\n",
    "max_len = len(df['tokens_words'].max())\n",
    "\n",
    "# Pad the token sequences with zeros to match the maximum length\n",
    "df['tokens_words'] = df['tokens_words'].apply(lambda tokens: tokens + [0] * (max_len - len(tokens)))\n",
    "\n",
    "# Convert the 'word_sentiment' column from string representation to list\n",
    "df['word_sentiment'] = df['word_sentiment'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Pad the sentiment scores with zeros to match the maximum length\n",
    "df['word_sentiment'] = df['word_sentiment'].apply(lambda tokens: tokens + [0] * (max_len - len(tokens)))\n",
    "\n",
    "# Display the first token sequence after padding\n",
    "display(df['tokens_words'][0])\n",
    "\n",
    "# Display the first sentiment scores after padding\n",
    "display(df['word_sentiment'][0])\n",
    "\n",
    "# Convert the token sequences to word features using Word2Vec model\n",
    "sentences = df['tokens_words'].tolist()\n",
    "model = Word2Vec(sentences, vector_size=25, window=5, min_count=1, workers=4)\n",
    "\n",
    "def extract_text_features(token_sequences, model):\n",
    "    text_features = []\n",
    "    for tokens in token_sequences:\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        if vectors:\n",
    "            feature = np.mean(vectors, axis=0)  # Compute the mean of word vectors\n",
    "            text_features.append(feature)\n",
    "        else:\n",
    "            text_features.append(np.zeros(model.vector_size))  # Handle tokens with no word vectors\n",
    "    return np.array(text_features)\n",
    "\n",
    "# Extract word features for each token sequence\n",
    "word_features = extract_text_features(df['tokens_words'], model)\n",
    "\n",
    "# Assign the word features to the 'word_features' column\n",
    "df['word_features'] = list(word_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0c82bc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgid</th>\n",
       "      <th>split</th>\n",
       "      <th>filename</th>\n",
       "      <th>successful</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>raw</th>\n",
       "      <th>processed_caption</th>\n",
       "      <th>tokens_process</th>\n",
       "      <th>text_feature</th>\n",
       "      <th>image_path</th>\n",
       "      <th>preprocessed_image</th>\n",
       "      <th>visual_features</th>\n",
       "      <th>tokens_words</th>\n",
       "      <th>word_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'plate', 'of', 'delicious', 'food', 'inc...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>a plate of delicious food including French fries.</td>\n",
       "      <td>a plate of delicious food including french fries.</td>\n",
       "      <td>['plate', 'delicious', 'food', 'including', 'f...</td>\n",
       "      <td>[0.14435287, -0.39209256, 0.60560834, -0.35789...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....</td>\n",
       "      <td>[tensor(2.9000, dtype=torch.float64), tensor(-...</td>\n",
       "      <td>[a, plate, of, delicious, food, including, fre...</td>\n",
       "      <td>[1.1180228, -0.37944388, -0.084031604, -0.7050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['french', 'fries', 'are', 'not', 'a', 'health...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>French fries are not a healthy food but it is ...</td>\n",
       "      <td>french fries are not a healthy food but it is ...</td>\n",
       "      <td>['french', 'fries', 'healthy', 'food', 'excell...</td>\n",
       "      <td>[0.12529851, -0.392354, 0.5898358, -0.3611061,...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....</td>\n",
       "      <td>[tensor(2.9000, dtype=torch.float64), tensor(-...</td>\n",
       "      <td>[french, fries, are, not, a, healthy, food, bu...</td>\n",
       "      <td>[1.0003605, -0.35224086, 0.08382912, -0.792501...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['the', 'plate', 'has', 'one', 'of', 'my', 'fa...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>The plate has one of my favorite foods on it, ...</td>\n",
       "      <td>the plate has one of my favorite foods on it, ...</td>\n",
       "      <td>['plate', 'one', 'favorite', 'foods', ',', 'fr...</td>\n",
       "      <td>[0.11975639, -0.33411312, 0.6227862, -0.328492...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....</td>\n",
       "      <td>[tensor(2.9000, dtype=torch.float64), tensor(-...</td>\n",
       "      <td>[the, plate, has, one, of, my, favorite, foods...</td>\n",
       "      <td>[0.96932065, -0.35699126, -0.0015931309, -0.71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['it', 'was', 'disgusting', 'food', 'not', 'ju...</td>\n",
       "      <td>[0.0, 0.0, 1, 1, 0.0, 0.0, 1, 1, 0, 0, 0, 0, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>It was disgusting food, not just bad food.</td>\n",
       "      <td>it was disgusting food, not just bad food.</td>\n",
       "      <td>['disgusting', 'food', ',', 'bad', 'food', '.'...</td>\n",
       "      <td>[0.13780436, -0.407391, 0.60546184, -0.3719354...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....</td>\n",
       "      <td>[tensor(2.9000, dtype=torch.float64), tensor(-...</td>\n",
       "      <td>[it, was, disgusting, food, not, just, bad, fo...</td>\n",
       "      <td>[1.2341136, -0.51621985, -0.05004932, -0.67997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'plate', 'of', 'disgusting', 'food', 'fo...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>A plate of disgusting food found at a diner.</td>\n",
       "      <td>a plate of disgusting food found at a diner.</td>\n",
       "      <td>['plate', 'disgusting', 'food', 'found', 'dine...</td>\n",
       "      <td>[0.13693358, -0.41861236, 0.6273109, -0.348105...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....</td>\n",
       "      <td>[tensor(2.9000, dtype=torch.float64), tensor(-...</td>\n",
       "      <td>[a, plate, of, disgusting, food, found, at, a,...</td>\n",
       "      <td>[1.096952, -0.32524014, -0.039901063, -0.80140...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39176</th>\n",
       "      <td>23481</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000467249.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['i', 'think', 'there', 'is', 'a', 'broken', '...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>I think there is a broken window on the clock ...</td>\n",
       "      <td>i think there is a broken window on the clock ...</td>\n",
       "      <td>['think', 'broken', 'window', 'clock', 'tower'...</td>\n",
       "      <td>[-0.01054071, -0.3633859, 0.83063734, -0.34671...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....</td>\n",
       "      <td>[tensor(-3.9285, dtype=torch.float64), tensor(...</td>\n",
       "      <td>[i, think, there, is, a, broken, window, on, t...</td>\n",
       "      <td>[1.0628843, -0.26338357, 0.07811466, -0.878417...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39177</th>\n",
       "      <td>23481</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000467249.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'large', 'abandoned', 'building', 'with'...</td>\n",
       "      <td>[0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>a large abandoned building with a pointy clock...</td>\n",
       "      <td>a large abandoned building with a pointy clock...</td>\n",
       "      <td>['large', 'abandoned', 'building', 'pointy', '...</td>\n",
       "      <td>[-0.03648873, -0.3410362, 0.845356, -0.3553167...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....</td>\n",
       "      <td>[tensor(-3.9285, dtype=torch.float64), tensor(...</td>\n",
       "      <td>[a, large, abandoned, building, with, a, point...</td>\n",
       "      <td>[0.80504346, -0.2019368, 0.17393322, -0.974013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39178</th>\n",
       "      <td>23481</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000467249.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'dirty', 'window', 'sits', 'idly', 'stil...</td>\n",
       "      <td>[0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>A dirty window sits idly still, beckoning unto...</td>\n",
       "      <td>a dirty window sits idly still, beckoning unto...</td>\n",
       "      <td>['dirty', 'window', 'sits', 'idly', 'still', '...</td>\n",
       "      <td>[-0.01766033, -0.25001994, 0.64878535, -0.2519...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....</td>\n",
       "      <td>[tensor(-3.9285, dtype=torch.float64), tensor(...</td>\n",
       "      <td>[a, dirty, window, sits, idly, still, beckonin...</td>\n",
       "      <td>[1.0257969, -0.20746478, -0.04126925, -0.82713...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39179</th>\n",
       "      <td>23481</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000467249.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['i', 'think', 'there', 'is', 'a', 'broken', '...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>I think there is a broken window on the clock ...</td>\n",
       "      <td>i think there is a broken window on the clock ...</td>\n",
       "      <td>['think', 'broken', 'window', 'clock', 'tower'...</td>\n",
       "      <td>[-0.01054071, -0.3633859, 0.83063734, -0.34671...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....</td>\n",
       "      <td>[tensor(-3.9285, dtype=torch.float64), tensor(...</td>\n",
       "      <td>[i, think, there, is, a, broken, window, on, t...</td>\n",
       "      <td>[1.0628843, -0.26338357, 0.07811466, -0.878417...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39180</th>\n",
       "      <td>23481</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000467249.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'large', 'abandoned', 'building', 'with'...</td>\n",
       "      <td>[0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>a large abandoned building with a pointy clock...</td>\n",
       "      <td>a large abandoned building with a pointy clock...</td>\n",
       "      <td>['large', 'abandoned', 'building', 'pointy', '...</td>\n",
       "      <td>[-0.03648873, -0.3410362, 0.845356, -0.3553167...</td>\n",
       "      <td>/content/drive/MyDrive/sentiment/sentiment_ima...</td>\n",
       "      <td>tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....</td>\n",
       "      <td>[tensor(-3.9285, dtype=torch.float64), tensor(...</td>\n",
       "      <td>[a, large, abandoned, building, with, a, point...</td>\n",
       "      <td>[0.80504346, -0.2019368, 0.17393322, -0.974013...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9472 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       imgid  split                       filename  successful  \\\n",
       "0      31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "1      31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "2      31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "3      31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "4      31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "...      ...    ...                            ...         ...   \n",
       "39176  23481   test  COCO_val2014_000000467249.jpg           1   \n",
       "39177  23481   test  COCO_val2014_000000467249.jpg           1   \n",
       "39178  23481   test  COCO_val2014_000000467249.jpg           1   \n",
       "39179  23481   test  COCO_val2014_000000467249.jpg           1   \n",
       "39180  23481   test  COCO_val2014_000000467249.jpg           1   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      ['a', 'plate', 'of', 'delicious', 'food', 'inc...   \n",
       "1      ['french', 'fries', 'are', 'not', 'a', 'health...   \n",
       "2      ['the', 'plate', 'has', 'one', 'of', 'my', 'fa...   \n",
       "3      ['it', 'was', 'disgusting', 'food', 'not', 'ju...   \n",
       "4      ['a', 'plate', 'of', 'disgusting', 'food', 'fo...   \n",
       "...                                                  ...   \n",
       "39176  ['i', 'think', 'there', 'is', 'a', 'broken', '...   \n",
       "39177  ['a', 'large', 'abandoned', 'building', 'with'...   \n",
       "39178  ['a', 'dirty', 'window', 'sits', 'idly', 'stil...   \n",
       "39179  ['i', 'think', 'there', 'is', 'a', 'broken', '...   \n",
       "39180  ['a', 'large', 'abandoned', 'building', 'with'...   \n",
       "\n",
       "                                          word_sentiment  sentiment  \\\n",
       "0      [0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0, 0, 0, ...          1   \n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0,...          1   \n",
       "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0,...          1   \n",
       "3      [0.0, 0.0, 1, 1, 0.0, 0.0, 1, 1, 0, 0, 0, 0, 0...          0   \n",
       "4      [0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0, 0...          0   \n",
       "...                                                  ...        ...   \n",
       "39176  [0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0,...          0   \n",
       "39177  [0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...          0   \n",
       "39178  [0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 1...          0   \n",
       "39179  [0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0,...          0   \n",
       "39180  [0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...          0   \n",
       "\n",
       "                                                     raw  \\\n",
       "0      a plate of delicious food including French fries.   \n",
       "1      French fries are not a healthy food but it is ...   \n",
       "2      The plate has one of my favorite foods on it, ...   \n",
       "3             It was disgusting food, not just bad food.   \n",
       "4           A plate of disgusting food found at a diner.   \n",
       "...                                                  ...   \n",
       "39176  I think there is a broken window on the clock ...   \n",
       "39177  a large abandoned building with a pointy clock...   \n",
       "39178  A dirty window sits idly still, beckoning unto...   \n",
       "39179  I think there is a broken window on the clock ...   \n",
       "39180  a large abandoned building with a pointy clock...   \n",
       "\n",
       "                                       processed_caption  \\\n",
       "0      a plate of delicious food including french fries.   \n",
       "1      french fries are not a healthy food but it is ...   \n",
       "2      the plate has one of my favorite foods on it, ...   \n",
       "3             it was disgusting food, not just bad food.   \n",
       "4           a plate of disgusting food found at a diner.   \n",
       "...                                                  ...   \n",
       "39176  i think there is a broken window on the clock ...   \n",
       "39177  a large abandoned building with a pointy clock...   \n",
       "39178  a dirty window sits idly still, beckoning unto...   \n",
       "39179  i think there is a broken window on the clock ...   \n",
       "39180  a large abandoned building with a pointy clock...   \n",
       "\n",
       "                                          tokens_process  \\\n",
       "0      ['plate', 'delicious', 'food', 'including', 'f...   \n",
       "1      ['french', 'fries', 'healthy', 'food', 'excell...   \n",
       "2      ['plate', 'one', 'favorite', 'foods', ',', 'fr...   \n",
       "3      ['disgusting', 'food', ',', 'bad', 'food', '.'...   \n",
       "4      ['plate', 'disgusting', 'food', 'found', 'dine...   \n",
       "...                                                  ...   \n",
       "39176  ['think', 'broken', 'window', 'clock', 'tower'...   \n",
       "39177  ['large', 'abandoned', 'building', 'pointy', '...   \n",
       "39178  ['dirty', 'window', 'sits', 'idly', 'still', '...   \n",
       "39179  ['think', 'broken', 'window', 'clock', 'tower'...   \n",
       "39180  ['large', 'abandoned', 'building', 'pointy', '...   \n",
       "\n",
       "                                            text_feature  \\\n",
       "0      [0.14435287, -0.39209256, 0.60560834, -0.35789...   \n",
       "1      [0.12529851, -0.392354, 0.5898358, -0.3611061,...   \n",
       "2      [0.11975639, -0.33411312, 0.6227862, -0.328492...   \n",
       "3      [0.13780436, -0.407391, 0.60546184, -0.3719354...   \n",
       "4      [0.13693358, -0.41861236, 0.6273109, -0.348105...   \n",
       "...                                                  ...   \n",
       "39176  [-0.01054071, -0.3633859, 0.83063734, -0.34671...   \n",
       "39177  [-0.03648873, -0.3410362, 0.845356, -0.3553167...   \n",
       "39178  [-0.01766033, -0.25001994, 0.64878535, -0.2519...   \n",
       "39179  [-0.01054071, -0.3633859, 0.83063734, -0.34671...   \n",
       "39180  [-0.03648873, -0.3410362, 0.845356, -0.3553167...   \n",
       "\n",
       "                                              image_path  \\\n",
       "0      /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "1      /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "2      /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "3      /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "4      /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "...                                                  ...   \n",
       "39176  /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "39177  /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "39178  /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "39179  /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "39180  /content/drive/MyDrive/sentiment/sentiment_ima...   \n",
       "\n",
       "                                      preprocessed_image  \\\n",
       "0      tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....   \n",
       "1      tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....   \n",
       "2      tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....   \n",
       "3      tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....   \n",
       "4      tensor([[[-1.5699, -1.5357, -1.5185,  ..., -1....   \n",
       "...                                                  ...   \n",
       "39176  tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....   \n",
       "39177  tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....   \n",
       "39178  tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....   \n",
       "39179  tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....   \n",
       "39180  tensor([[[ 0.3138,  0.3138,  0.3309,  ...,  0....   \n",
       "\n",
       "                                         visual_features  \\\n",
       "0      [tensor(2.9000, dtype=torch.float64), tensor(-...   \n",
       "1      [tensor(2.9000, dtype=torch.float64), tensor(-...   \n",
       "2      [tensor(2.9000, dtype=torch.float64), tensor(-...   \n",
       "3      [tensor(2.9000, dtype=torch.float64), tensor(-...   \n",
       "4      [tensor(2.9000, dtype=torch.float64), tensor(-...   \n",
       "...                                                  ...   \n",
       "39176  [tensor(-3.9285, dtype=torch.float64), tensor(...   \n",
       "39177  [tensor(-3.9285, dtype=torch.float64), tensor(...   \n",
       "39178  [tensor(-3.9285, dtype=torch.float64), tensor(...   \n",
       "39179  [tensor(-3.9285, dtype=torch.float64), tensor(...   \n",
       "39180  [tensor(-3.9285, dtype=torch.float64), tensor(...   \n",
       "\n",
       "                                            tokens_words  \\\n",
       "0      [a, plate, of, delicious, food, including, fre...   \n",
       "1      [french, fries, are, not, a, healthy, food, bu...   \n",
       "2      [the, plate, has, one, of, my, favorite, foods...   \n",
       "3      [it, was, disgusting, food, not, just, bad, fo...   \n",
       "4      [a, plate, of, disgusting, food, found, at, a,...   \n",
       "...                                                  ...   \n",
       "39176  [i, think, there, is, a, broken, window, on, t...   \n",
       "39177  [a, large, abandoned, building, with, a, point...   \n",
       "39178  [a, dirty, window, sits, idly, still, beckonin...   \n",
       "39179  [i, think, there, is, a, broken, window, on, t...   \n",
       "39180  [a, large, abandoned, building, with, a, point...   \n",
       "\n",
       "                                           word_features  \n",
       "0      [1.1180228, -0.37944388, -0.084031604, -0.7050...  \n",
       "1      [1.0003605, -0.35224086, 0.08382912, -0.792501...  \n",
       "2      [0.96932065, -0.35699126, -0.0015931309, -0.71...  \n",
       "3      [1.2341136, -0.51621985, -0.05004932, -0.67997...  \n",
       "4      [1.096952, -0.32524014, -0.039901063, -0.80140...  \n",
       "...                                                  ...  \n",
       "39176  [1.0628843, -0.26338357, 0.07811466, -0.878417...  \n",
       "39177  [0.80504346, -0.2019368, 0.17393322, -0.974013...  \n",
       "39178  [1.0257969, -0.20746478, -0.04126925, -0.82713...  \n",
       "39179  [1.0628843, -0.26338357, 0.07811466, -0.878417...  \n",
       "39180  [0.80504346, -0.2019368, 0.17393322, -0.974013...  \n",
       "\n",
       "[9472 rows x 16 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96859db1",
   "metadata": {},
   "source": [
    "# Task 01: PyTorch Model for Sentence level Multimodal Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2afd8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultimodalNetwork(nn.Module):\n",
    "    def __init__(self, text_input_size, image_input_size, hidden_sizes, output_size):\n",
    "        super(MultimodalNetwork, self).__init__()\n",
    "\n",
    "        # Textual network\n",
    "        self.text_net = nn.Sequential(\n",
    "            nn.Linear(text_input_size, hidden_sizes[0]),  # Input size to the first hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[2], hidden_sizes[7]),  # Output size of the textual network\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Visual network\n",
    "        self.image_net = nn.Sequential(\n",
    "            nn.Linear(image_input_size, hidden_sizes[4]),  # Input size to the first hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[4], hidden_sizes[5]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[5], hidden_sizes[6]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[6], hidden_sizes[7]),  # Output size of the visual network\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Combined network\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(hidden_sizes[7] + hidden_sizes[7], hidden_sizes[8]),  # Input size: concatenation of textual and visual output\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[8], hidden_sizes[9]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[9], hidden_sizes[10]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[10], hidden_sizes[11]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[11], output_size)  # Output size of the combined network\n",
    "        )\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        text_output = self.text_net(text_features.float())  # Convert to float data type\n",
    "        image_output = self.image_net(image_features.float())  # Convert to float data type\n",
    "        combined = torch.cat((text_output, image_output), dim=1)  # Concatenate the textual and visual outputs\n",
    "        output = self.combined_net(combined)  # Pass the concatenated output through the combined network\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "924a2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class YourDataset(Dataset):\n",
    "    def __init__(self, text_features, image_features, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with text features, image features, and labels.\n",
    "\n",
    "        Args:\n",
    "            text_features (list): List of text features.\n",
    "            image_features (list): List of image features.\n",
    "            labels (list): List of corresponding labels.\n",
    "        \"\"\"\n",
    "        self.text_features = text_features\n",
    "        self.image_features = image_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.text_features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing text tensor, image tensor, and label tensor.\n",
    "        \"\"\"\n",
    "        text = self.text_features[index]\n",
    "        image = self.image_features[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Convert to tensors\n",
    "        text_tensor = torch.Tensor(text)\n",
    "\n",
    "        if image is not None:\n",
    "            image_tensor = torch.Tensor(image)\n",
    "        else:\n",
    "            # Handle the case when image is None\n",
    "            image_tensor = torch.zeros_like(text_tensor)\n",
    "\n",
    "        label_tensor = torch.Tensor([label])\n",
    "\n",
    "        return text_tensor, image_tensor, label_tensor\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for batching the dataset.\n",
    "\n",
    "        Args:\n",
    "            batch (list): List of samples to batch.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing padded text, padded images, and padded labels.\n",
    "        \"\"\"\n",
    "        text_batch, image_batch, label_batch = zip(*batch)\n",
    "        text_padded = pad_sequence(text_batch, batch_first=True)\n",
    "\n",
    "        if image_batch[0].shape[0] > 0:\n",
    "            image_padded = pad_sequence(image_batch, batch_first=True)\n",
    "        else:\n",
    "            # Handle the case when all image tensors are empty\n",
    "            image_padded = torch.zeros_like(text_padded)\n",
    "\n",
    "        label_padded = torch.cat(label_batch).unsqueeze(1)  # Unsqueeze the label tensor\n",
    "\n",
    "        return text_padded, image_padded, label_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b13302ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the input sizes and other hyperparameters\n",
    "text_input_size = len(df['text_feature'].iloc[0])\n",
    "image_input_size = df['visual_features'].iloc[0].shape[0] # Assuming each 'visual_features' entry is a 1D array\n",
    "hidden_sizes = [512*2, 256*2, 512*2, 256*2, 128*2, 64*2, 128*2, 64*2, 32*2, 16*2, 8*2,4*2]\n",
    "output_size = 1 # Assuming a binary classification task\n",
    "\n",
    "# Define the hyperparameters for training\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 15\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assuming you have train_data, val_data as your training and validation sets\n",
    "train_text_features = train_data['text_feature'].values\n",
    "train_image_features = train_data['visual_features'].values\n",
    "train_labels = train_data['sentiment'].values\n",
    "\n",
    "# Assuming you have defined batch_size and other hyperparameters\n",
    "train_dataset = YourDataset(train_text_features, train_image_features, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "# Assuming you have train_data, val_data as your training and validation sets\n",
    "val_text_features = val_data['text_feature'].values\n",
    "val_image_features = val_data['visual_features'].values\n",
    "val_labels = val_data['sentiment'].values\n",
    "\n",
    "# Assuming you have defined batch_size and other hyperparameters\n",
    "val_dataset = YourDataset(val_text_features, val_image_features, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "# Check if GPU is available and choose device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create an instance of the MultimodalNetwork model\n",
    "model = MultimodalNetwork(text_input_size, image_input_size, hidden_sizes, output_size)\n",
    "model = model.to(device)  # Move the model to the same device as the input tensors\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f16b07",
   "metadata": {},
   "source": [
    "# TRAINING SENTENCE SENTIMENT MODEL:------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cbefc5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Train Loss: 0.6859, Train Accuracy: 0.5441\n",
      "Epoch 2/15\n",
      "Train Loss: 0.6800, Train Accuracy: 0.5401\n",
      "Epoch 3/15\n",
      "Train Loss: 0.6653, Train Accuracy: 0.5490\n",
      "Epoch 4/15\n",
      "Train Loss: 0.6452, Train Accuracy: 0.5653\n",
      "Epoch 5/15\n",
      "Train Loss: 0.5909, Train Accuracy: 0.6451\n",
      "Epoch 6/15\n",
      "Train Loss: 0.4665, Train Accuracy: 0.7661\n",
      "Epoch 7/15\n",
      "Train Loss: 0.3982, Train Accuracy: 0.8037\n",
      "Epoch 8/15\n",
      "Train Loss: 0.3497, Train Accuracy: 0.8390\n",
      "Epoch 9/15\n",
      "Train Loss: 0.3301, Train Accuracy: 0.8436\n",
      "Epoch 10/15\n",
      "Train Loss: 0.3071, Train Accuracy: 0.8565\n",
      "Epoch 11/15\n",
      "Train Loss: 0.2917, Train Accuracy: 0.8617\n",
      "Epoch 12/15\n",
      "Train Loss: 0.2749, Train Accuracy: 0.8716\n",
      "Epoch 13/15\n",
      "Train Loss: 0.2568, Train Accuracy: 0.8765\n",
      "Epoch 14/15\n",
      "Train Loss: 0.2260, Train Accuracy: 0.8885\n",
      "Epoch 15/15\n",
      "Train Loss: 0.2227, Train Accuracy: 0.8891\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "\n",
    "    for text_features, image_features, labels in train_loader:\n",
    "        text_features = text_features.to(device)\n",
    "        image_features = image_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text_features, image_features)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the training loss and correct predictions\n",
    "        train_loss += loss.item()\n",
    "        train_correct += torch.sum(torch.round(torch.sigmoid(outputs)).long() == labels.squeeze(0).long()).item()\n",
    "\n",
    "    # Compute average training loss and accuracy\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_correct / len(train_data)\n",
    "    \n",
    "    # Append training loss and accuracy to lists\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953094d",
   "metadata": {},
   "source": [
    "# TESTING  SENTENCE SENTIMENT MODEL:------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7cf6477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0026, Validation Accuracy: 0.0158\n",
      "Validation Loss: 0.0063, Validation Accuracy: 0.0296\n",
      "Validation Loss: 0.0033, Validation Accuracy: 0.0449\n",
      "Validation Loss: 0.0025, Validation Accuracy: 0.0607\n",
      "Validation Loss: 0.0029, Validation Accuracy: 0.0760\n",
      "Validation Loss: 0.0048, Validation Accuracy: 0.0918\n",
      "Validation Loss: 0.0032, Validation Accuracy: 0.1071\n",
      "Validation Loss: 0.0069, Validation Accuracy: 0.1208\n",
      "Validation Loss: 0.0029, Validation Accuracy: 0.1356\n",
      "Validation Loss: 0.0036, Validation Accuracy: 0.1509\n",
      "Validation Loss: 0.0033, Validation Accuracy: 0.1662\n",
      "Validation Loss: 0.0038, Validation Accuracy: 0.1815\n",
      "Validation Loss: 0.0041, Validation Accuracy: 0.1963\n",
      "Validation Loss: 0.0044, Validation Accuracy: 0.2106\n",
      "Validation Loss: 0.0015, Validation Accuracy: 0.2274\n",
      "Validation Loss: 0.0150, Validation Accuracy: 0.2412\n",
      "Validation Loss: 0.0038, Validation Accuracy: 0.2559\n",
      "Validation Loss: 0.0038, Validation Accuracy: 0.2712\n",
      "Validation Loss: 0.0026, Validation Accuracy: 0.2871\n",
      "Validation Loss: 0.0032, Validation Accuracy: 0.3029\n",
      "Validation Loss: 0.0035, Validation Accuracy: 0.3182\n",
      "Validation Loss: 0.0022, Validation Accuracy: 0.3351\n",
      "Validation Loss: 0.0053, Validation Accuracy: 0.3483\n",
      "Validation Loss: 0.0026, Validation Accuracy: 0.3636\n",
      "Validation Loss: 0.0027, Validation Accuracy: 0.3789\n",
      "Validation Loss: 0.0033, Validation Accuracy: 0.3947\n",
      "Validation Loss: 0.0030, Validation Accuracy: 0.4111\n",
      "Validation Loss: 0.0042, Validation Accuracy: 0.4269\n",
      "Validation Loss: 0.0036, Validation Accuracy: 0.4417\n",
      "Validation Loss: 0.0017, Validation Accuracy: 0.4586\n",
      "Validation Loss: 0.0072, Validation Accuracy: 0.4734\n",
      "Validation Loss: 0.0057, Validation Accuracy: 0.4881\n",
      "Validation Loss: 0.0039, Validation Accuracy: 0.5024\n",
      "Validation Loss: 0.0051, Validation Accuracy: 0.5156\n",
      "Validation Loss: 0.0020, Validation Accuracy: 0.5319\n",
      "Validation Loss: 0.0041, Validation Accuracy: 0.5462\n",
      "Validation Loss: 0.0028, Validation Accuracy: 0.5615\n",
      "Validation Loss: 0.0020, Validation Accuracy: 0.5778\n",
      "Validation Loss: 0.0054, Validation Accuracy: 0.5921\n",
      "Validation Loss: 0.0042, Validation Accuracy: 0.6063\n",
      "Validation Loss: 0.0024, Validation Accuracy: 0.6222\n",
      "Validation Loss: 0.0077, Validation Accuracy: 0.6348\n",
      "Validation Loss: 0.0033, Validation Accuracy: 0.6501\n",
      "Validation Loss: 0.0022, Validation Accuracy: 0.6670\n",
      "Validation Loss: 0.0054, Validation Accuracy: 0.6797\n",
      "Validation Loss: 0.0049, Validation Accuracy: 0.6945\n",
      "Validation Loss: 0.0030, Validation Accuracy: 0.7098\n",
      "Validation Loss: 0.0045, Validation Accuracy: 0.7230\n",
      "Validation Loss: 0.0022, Validation Accuracy: 0.7383\n",
      "Validation Loss: 0.0029, Validation Accuracy: 0.7530\n",
      "Validation Loss: 0.0023, Validation Accuracy: 0.7683\n",
      "Validation Loss: 0.0027, Validation Accuracy: 0.7842\n",
      "Validation Loss: 0.0032, Validation Accuracy: 0.7989\n",
      "Validation Loss: 0.0053, Validation Accuracy: 0.8132\n",
      "Validation Loss: 0.0046, Validation Accuracy: 0.8274\n",
      "Validation Loss: 0.0036, Validation Accuracy: 0.8422\n",
      "Validation Loss: 0.0027, Validation Accuracy: 0.8586\n",
      "Validation Loss: 0.0049, Validation Accuracy: 0.8739\n",
      "Validation Loss: 0.0044, Validation Accuracy: 0.8897\n",
      "Validation Loss: 0.0020, Validation Accuracy: 0.8934\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89       873\n",
      "           1       0.97      0.82      0.89      1022\n",
      "\n",
      "    accuracy                           0.89      1895\n",
      "   macro avg       0.90      0.90      0.89      1895\n",
      "weighted avg       0.91      0.89      0.89      1895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "val_correct = 0\n",
    "val_true_labels = []\n",
    "val_pred_labels = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text_features, image_features, labels in val_loader:\n",
    "        text_features = text_features.to(device)\n",
    "        image_features = image_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text_features, image_features)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Accumulate the validation loss and correct predictions\n",
    "        val_loss += loss.item()\n",
    "        val_correct += torch.sum(torch.round(torch.sigmoid(outputs)).long() == labels.squeeze(0).long()).item()\n",
    "\n",
    "        # Append true labels and predicted labels for evaluation\n",
    "        val_true_labels.extend(labels.squeeze(0).long().tolist())\n",
    "        val_pred_labels.extend(torch.round(torch.sigmoid(outputs)).long().tolist())\n",
    "\n",
    "    # Compute average validation loss and accuracy\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = val_correct / len(val_data)\n",
    "    \n",
    "    # Append validation loss and accuracy to lists\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print validation results\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Compute classification report\n",
    "report = classification_report(val_true_labels, val_pred_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8e6be13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, label='Train Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9eb088f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model_sentence.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460313af",
   "metadata": {},
   "source": [
    "# Task 02: PyTorch Model for Word level Sentiment Analysis\n",
    "#### Do word level sentiment analysis by making custom feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "802b1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalNetwork(nn.Module):\n",
    "    def __init__(self, input_text_size, input_image_size, hidden_sizes, output_size):\n",
    "        super(MultimodalNetwork, self).__init__()\n",
    "\n",
    "        # Textual network\n",
    "        self.text_net = nn.Sequential(\n",
    "            nn.Linear(input_text_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[3], hidden_sizes[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[4], hidden_sizes[5]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[5], hidden_sizes[6]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[6], hidden_sizes[7]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[7], hidden_sizes[8]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Visual network\n",
    "        self.image_net = nn.Sequential(\n",
    "            nn.Linear(input_image_size, hidden_sizes[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[3], hidden_sizes[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[3], hidden_sizes[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[3], hidden_sizes[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[3], hidden_sizes[3]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Combined network\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(hidden_sizes[3] + hidden_sizes[8], hidden_sizes[9]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[9], hidden_sizes[10]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[10], hidden_sizes[11]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[11], hidden_sizes[12]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[12], hidden_sizes[12]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[12], hidden_sizes[12]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[12], hidden_sizes[12]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[12], hidden_sizes[12]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[12], hidden_sizes[12]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[12], output_size)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        \"\"\"\n",
    "        Forward pass of the multimodal network.\n",
    "\n",
    "        Args:\n",
    "            text_features (torch.Tensor): Input textual features.\n",
    "            image_features (torch.Tensor): Input image features.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions.\n",
    "        \"\"\"\n",
    "        text_output = self.text_net(text_features.float())\n",
    "        image_output = self.image_net(image_features.float())\n",
    "        combined = torch.cat((text_output, image_output), dim=1)\n",
    "        output = self.combined_net(combined)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11adf725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class YourDataset(Dataset):\n",
    "    def __init__(self, text_features, image_features, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with text features, image features, and labels.\n",
    "\n",
    "        Args:\n",
    "            text_features (list): List of text features.\n",
    "            image_features (list): List of image features.\n",
    "            labels (list): List of corresponding labels.\n",
    "        \"\"\"\n",
    "        self.text_features = text_features\n",
    "        self.image_features = image_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.text_features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing text tensor, image tensor, and label tensor.\n",
    "        \"\"\"\n",
    "        text = self.text_features[index]\n",
    "        image = self.image_features[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        text_tensor = torch.Tensor(text)\n",
    "\n",
    "        if image is not None:\n",
    "            image_tensor = torch.Tensor(image)\n",
    "        else:\n",
    "            image_tensor = torch.zeros_like(text_tensor)\n",
    "\n",
    "        label_tensor = torch.Tensor(label)\n",
    "\n",
    "        return text_tensor, image_tensor, label_tensor\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for batching the dataset.\n",
    "\n",
    "        Args:\n",
    "            batch (list): List of samples to batch.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing padded text, padded images, and padded labels.\n",
    "        \"\"\"\n",
    "        text_batch, image_batch, label_batch = zip(*batch)\n",
    "        text_padded = pad_sequence(text_batch, batch_first=True)\n",
    "\n",
    "        if image_batch[0].shape[0] > 0:\n",
    "            image_padded = pad_sequence(image_batch, batch_first=True)\n",
    "        else:\n",
    "            image_padded = torch.zeros_like(text_padded)\n",
    "\n",
    "        labels = []\n",
    "        for label in label_batch:\n",
    "            if label.shape[0] > text_padded.shape[1]:\n",
    "                label = label[:text_padded.shape[1]]\n",
    "            labels.append(label)\n",
    "\n",
    "        label_padded = pad_sequence(labels, batch_first=True)\n",
    "\n",
    "        return text_padded, image_padded, label_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "21968579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input sizes, hidden sizes, output size, and other hyperparameters\n",
    "input_text_size = len(df['word_features'].iloc[0])\n",
    "input_image_size = df['visual_features'].iloc[0].shape[0]\n",
    "hidden_sizes = [512*2, 256*2, 128*2, 256*2, 128*2, 64*2, 32*2, 64*2, 32*2, 16*2, 8*2, 4*2, 25*2]\n",
    "output_size = 25\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the training features, labels, and create the training dataset and data loader\n",
    "train_features1 = train['word_features'].values\n",
    "train_features2 = train['visual_features'].values\n",
    "train_labels = train['word_sentiment'].values\n",
    "train_dataset = YourDataset(train_features1, train_features2, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "# Get the testing features, labels, and create the testing dataset and data loader\n",
    "test_features1 = test['word_features'].values\n",
    "test_features2 = test['visual_features'].values\n",
    "test_labels = test['word_sentiment'].values\n",
    "test_dataset = YourDataset(test_features1, test_features2, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "# Set the device to CUDA if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "model = MultimodalNetwork(input_text_size, input_image_size, hidden_sizes, output_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127f7c8",
   "metadata": {},
   "source": [
    "#  TRAINING WORDS SENTIMENT MODEL :----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6bc4c66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.2883, Train Accuracy: 0.8934\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2445, Train Accuracy: 0.9041\n",
      "Epoch 3/10\n",
      "Train Loss: 0.2369, Train Accuracy: 0.9049\n",
      "Epoch 4/10\n",
      "Train Loss: 0.2318, Train Accuracy: 0.9055\n",
      "Epoch 5/10\n",
      "Train Loss: 0.2279, Train Accuracy: 0.9078\n",
      "Epoch 6/10\n",
      "Train Loss: 0.2261, Train Accuracy: 0.9089\n",
      "Epoch 7/10\n",
      "Train Loss: 0.2217, Train Accuracy: 0.9107\n",
      "Epoch 8/10\n",
      "Train Loss: 0.2179, Train Accuracy: 0.9119\n",
      "Epoch 9/10\n",
      "Train Loss: 0.2178, Train Accuracy: 0.9124\n",
      "Epoch 10/10\n",
      "Train Loss: 0.2134, Train Accuracy: 0.9135\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0\n",
    "    for text_features, image_features, labels in train_loader:\n",
    "        # Move data to the appropriate device (CPU or GPU)\n",
    "        text_features = text_features.to(device)\n",
    "        image_features = image_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text_features, image_features)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the training loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        train_acc += torch.sum(torch.round(torch.sigmoid(outputs)).long() == labels.long()).item()\n",
    "\n",
    "    # Compute average training loss and accuracy for the epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_acc / (len(train) * output_size)\n",
    "    \n",
    "    # Store the loss and accuracy values\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Print epoch information\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training accuracy\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0134cd",
   "metadata": {},
   "source": [
    "#  TESTING WORDS SENTIMENT MODEL :----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7a3f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2129, Test Accuracy: 0.9159\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.66      0.66      0.66       774\n",
      "           2       0.64      0.65      0.64       782\n",
      "           3       0.00      0.00      0.00       234\n",
      "           4       0.00      0.00      0.00       227\n",
      "           5       0.00      0.00      0.00       239\n",
      "           6       0.00      0.00      0.00       265\n",
      "           7       0.56      0.09      0.16       299\n",
      "           8       0.52      0.27      0.35       335\n",
      "           9       0.00      0.00      0.00       353\n",
      "          10       0.00      0.00      0.00       274\n",
      "          11       0.00      0.00      0.00       163\n",
      "          12       0.00      0.00      0.00       124\n",
      "          13       0.00      0.00      0.00        96\n",
      "          14       0.00      0.00      0.00        78\n",
      "          15       0.00      0.00      0.00        45\n",
      "          16       0.00      0.00      0.00        31\n",
      "          17       0.00      0.00      0.00        28\n",
      "          18       0.00      0.00      0.00        19\n",
      "          19       0.00      0.00      0.00        10\n",
      "          20       0.00      0.00      0.00         6\n",
      "          21       0.00      0.00      0.00         7\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       0.00      0.00      0.00         4\n",
      "          24       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.64      0.25      0.36      4465\n",
      "   macro avg       0.10      0.07      0.07      4465\n",
      "weighted avg       0.30      0.25      0.26      4465\n",
      " samples avg       0.32      0.27      0.29      4465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FAHAD MAQBOOL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\FAHAD MAQBOOL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\FAHAD MAQBOOL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluation variables\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_acc = 0\n",
    "test_true_labels = []\n",
    "test_pred_labels = []\n",
    "\n",
    "# Disable gradient calculation during evaluation\n",
    "with torch.no_grad():\n",
    "    for text_features, image_features, labels in test_loader:\n",
    "        text_features = text_features.to(device)\n",
    "        image_features = image_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text_features, image_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Accumulate the test loss and accuracy\n",
    "        test_loss += loss.item()\n",
    "        test_acc += torch.sum(torch.round(torch.sigmoid(outputs)).long() == labels.long()).item()\n",
    "\n",
    "        # Append true labels and predicted labels for evaluation\n",
    "        test_true_labels.extend(labels.squeeze(0).long().tolist())\n",
    "        test_pred_labels.extend(torch.round(torch.sigmoid(outputs)).long().tolist())\n",
    "\n",
    "    # Compute average test loss and accuracy\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = test_acc / (len(test) * output_size)\n",
    "\n",
    "    # Print test results\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Compute classification report\n",
    "report = classification_report(test_true_labels, test_pred_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ffc94",
   "metadata": {},
   "source": [
    "# Task 03: PyTorch Model for Positive Sentiment Caption Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5cd58f",
   "metadata": {},
   "source": [
    "##### The task at hand is to create a PyTorch model that can generate positive sentiment captions by training it on a large dataset of positive captions. This can be achieved by fine-tuning a pre-existing model or creating a new one from scratch. The ultimate goal is to develop a model that can generate high-quality, positive captions that accurately reflect the content of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2de0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: cap.jpg\n",
      "Caption: a young boy holding a red and white fire hydrant in his hand.\n",
      "\n",
      "\"I don't know what to do,\" he said. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Load the GPT-2 tokenizer and language model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "lm_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lm_model.to(device)\n",
    "\n",
    "def fine_tune_positive_model(csv_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Get the captions and labels\n",
    "    captions = df[\"raw\"].tolist()\n",
    "    labels = df[\"sentiment\"].tolist()\n",
    "\n",
    "    # Filter out captions with label 1 (positive)\n",
    "    positive_captions = [caption for caption, label in zip(captions, labels) if label == 1]\n",
    "\n",
    "    # Tokenize the positive caption dataset\n",
    "    tokenized_dataset = tokenizer.batch_encode_plus(\n",
    "        positive_captions,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    lm_model.save_pretrained(\"enhanced_model\")\n",
    "\n",
    "def enhance_positive_caption(caption):\n",
    "    # Load the enhanced model\n",
    "    enhanced_model = GPT2LMHeadModel.from_pretrained(\"enhanced_model\")\n",
    "    enhanced_model.to(device)\n",
    "\n",
    "    # Encode the input caption\n",
    "    encoded_caption = tokenizer.encode(caption, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones_like(encoded_caption).to(device)\n",
    "\n",
    "    # Generate multiple enhanced captions\n",
    "    generated_output = enhanced_model.generate(\n",
    "        encoded_caption,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=30,\n",
    "        num_beams=8,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_return_sequences=2\n",
    "    )\n",
    "\n",
    "    enhanced_positive_captions = []\n",
    "    for output in generated_output:\n",
    "        generated_caption = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        enhanced_positive_captions.append(generated_caption)\n",
    "    return enhanced_positive_captions\n",
    "\n",
    "# Load the Vision Encoder-Decoder model, image processor, and GPT-2 language model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "lm_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "lm_model.to(device)\n",
    "\n",
    "max_length = 30\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "\n",
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        # Open the image and convert it to RGB if necessary\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        images.append(i_image)\n",
    "\n",
    "    # Process the images using the ViT image processor\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    # Generate captions using the Vision Encoder-Decoder model\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    # Filter out negative captions\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    positive_preds = [pred.strip() for pred in preds if \"negative\" not in pred]\n",
    "\n",
    "    # Select the best positive caption\n",
    "    best_positive_caption = max(positive_preds, key=len)\n",
    "\n",
    "    # Generate enhanced positive captions\n",
    "    enhanced_captions = enhance_positive_caption(best_positive_caption)\n",
    "\n",
    "    return enhanced_captions\n",
    "\n",
    "# Fine-tune the GPT-2 model using positive captions from a CSV file\n",
    "fine_tune_positive_model('caption.csv')\n",
    "\n",
    "# Example usage\n",
    "image_paths = [\"./sentiment/sentiment_images/COCO_val2014_000000032284.jpg\"]\n",
    "enhanced_captions = predict_step(image_paths)\n",
    "\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    print(\"Image:\", image_path)\n",
    "    print(\"Caption:\", enhanced_captions[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e643b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
